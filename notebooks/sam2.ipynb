{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "",
   "id": "6b97d264b59fd1e5"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-15T09:11:53.858771Z",
     "start_time": "2024-10-15T09:11:53.343012Z"
    }
   },
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "#os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "#import numpy as np\n",
    "#import torch\n",
    "\n",
    "#from od3d.cv.io import read_image\n",
    "#from od3d.cv.visual.show import show_img\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2 \n",
    "# `video_dir` a directory of JPEG frames with filenames like `<frame_index>.jpg`\n",
    "video_dir = \"/data/lmbraid19/sommerl/datasets/Objectron/frames/bike/batch_0_2/\"\n",
    "video_dir = \"/home/sommerl/video/batch_0_0/\"\n",
    "# scan all the JPEG frame names in this directory\n",
    "frame_names = [\n",
    "    p for p in os.listdir(video_dir)\n",
    "    if os.path.splitext(p)[-1] in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"]\n",
    "]\n",
    "frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
    "\n",
    "frame_idx = 0\n",
    "fpath = os.path.join(video_dir, frame_names[frame_idx])\n",
    "print(fpath)\n",
    "\n",
    "#img = Image.open(fpath)\n",
    "#img = read_image(fpath)\n",
    "#show_img(img)\n",
    "\n",
    "#img.shape\n",
    "# "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sommerl/video/batch_0_0/0.jpg\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T14:44:48.602492Z",
     "start_time": "2024-10-14T14:44:48.597819Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1a97982c91595024",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T14:44:55.921115Z",
     "start_time": "2024-10-14T14:44:51.942184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from sam2.sam2_video_predictor import SAM2VideoPredictor\n",
    "\n",
    "predictor = SAM2VideoPredictor.from_pretrained(\"facebook/sam2-hiera-large\")\n",
    "mask_generator = SAM2AutomaticMaskGenerator.from_pretrained(\"facebook/sam2-hiera-large\")\n"
   ],
   "id": "84c778ceb2640055",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T12:57:43.024660Z",
     "start_time": "2024-10-14T12:57:22.037614Z"
    }
   },
   "cell_type": "code",
   "source": "inference_state = predictor.init_state(video_path=video_dir)",
   "id": "9fa2cdbdb0c110b5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 318/318 [00:18<00:00, 17.36it/s]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T14:48:43.279349Z",
     "start_time": "2024-10-14T14:48:41.028966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
    "import torch\n",
    "mask_generator = SAM2AutomaticMaskGenerator.from_pretrained(\"facebook/sam2-hiera-large\", device=torch.device(\"cuda\"))"
   ],
   "id": "8a4e21d189e04739",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T14:47:59.452362Z",
     "start_time": "2024-10-14T14:47:59.410413Z"
    }
   },
   "cell_type": "code",
   "source": "mask_generator.cuda()",
   "id": "630a380bd9808eb8",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SAM2AutomaticMaskGenerator' object has no attribute 'cuda'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmask_generator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m()\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'SAM2AutomaticMaskGenerator' object has no attribute 'cuda'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-14T14:48:56.499115Z"
    }
   },
   "cell_type": "code",
   "source": "masks = mask_generator.generate(img.permute(2, 0, 1).numpy())\n",
   "id": "9bce94b39184043d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T15:07:50.737357Z",
     "start_time": "2024-10-14T15:07:48.569280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "predictor = SAM2ImagePredictor.from_pretrained(\"facebook/sam2-hiera-large\")\n",
    "\n"
   ],
   "id": "709773c618c343b3",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T09:12:22.445588Z",
     "start_time": "2024-10-15T09:11:59.906146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from sam2.sam2_video_predictor import SAM2VideoPredictor\n",
    "\n",
    "predictor = SAM2VideoPredictor.from_pretrained(\"facebook/sam2-hiera-large\")\n",
    "\n",
    "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    state = predictor.init_state(video_dir)"
   ],
   "id": "a06b8bf353dc7c39",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 318/318 [00:17<00:00, 18.07it/s]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "\n",
    "    # add new prompts and instantly get the output on the same frame\n",
    "    frame_idx, object_ids, masks = predictor.add_new_points_or_box(state, <your_prompts>):"
   ],
   "id": "95d720d8ebfc7ccf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T09:13:09.474394Z",
     "start_time": "2024-10-15T09:12:23.401868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from sam2.sam2_video_predictor import SAM2VideoPredictor\n",
    "import numpy as np\n",
    "\n",
    "predictor = SAM2VideoPredictor.from_pretrained(\"facebook/sam2-hiera-large\")\n",
    "masks_all = []\n",
    "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    state = predictor.init_state(video_dir)\n",
    "\n",
    "    # add new prompts and instantly get the output on the same frame\n",
    "    frame_idx, object_ids, masks = predictor.add_new_points_or_box(state, \n",
    "                        frame_idx=0, \n",
    "                        obj_id=0, points=np.array([[720, 960]]), labels=np.array([1]))\n",
    "\n",
    "    # propagate the prompts to get masklets throughout the video\n",
    "    for frame_idx, object_ids, masks in predictor.propagate_in_video(state):\n",
    "        masks_all.append(masks)"
   ],
   "id": "4ba0383cb50b2ed6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 318/318 [00:17<00:00, 17.85it/s]\n",
      "/home/sommerl/PycharmProjects/od3d/venv_od3d/lib/python3.10/site-packages/sam2/sam2_video_predictor.py:873: UserWarning: cannot import name '_C' from 'sam2' (/home/sommerl/PycharmProjects/od3d/venv_od3d/lib/python3.10/site-packages/sam2/__init__.py)\n",
      "\n",
      "Skipping the post-processing step due to the error above. You can still use SAM 2 and it's OK to ignore the error above, although some post-processing functionality may be limited (which doesn't affect the results in most cases; see https://github.com/facebookresearch/segment-anything-2/blob/main/INSTALL.md).\n",
      "  pred_masks_gpu = fill_holes_in_mask_scores(\n",
      "propagate in video: 100%|██████████| 318/318 [00:24<00:00, 13.07it/s]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T09:16:31.585242Z",
     "start_time": "2024-10-15T09:16:27.782527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "out_dir = Path(\"/home/sommerl/video/out_sam2_cu12\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "for i in range(len(masks_all)):\n",
    "    img = ((masks_all[i][0][0] > 0.5) * 255).detach().cpu().numpy().astype(np.uint8)\n",
    "    fpath = out_dir.joinpath(f\"{i}.png\")\n",
    "    cv2.imwrite(str(fpath), img)\n",
    "    \n",
    "    # show_img(masks_all[i][0] > 0.5, fpath=out_dir.joinpath(f\"{i}.png\"))"
   ],
   "id": "66601f7ed0f692fd",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T09:16:21.295844Z",
     "start_time": "2024-10-15T09:16:21.290195Z"
    }
   },
   "cell_type": "code",
   "source": "img.dtype",
   "id": "aab10c6ec6cba9fc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T14:59:14.225632Z",
     "start_time": "2024-10-14T14:59:14.072280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    predictor.set_image(img.permute(1, 2, 0).contiguous().detach().cpu().numpy(),)\n",
    "    # masks, _, _ = predictor.predict() # <input_prompts>\n",
    "    masks, scores, logits = predictor.predict(\n",
    "        point_coords=np.array([[720, 960]]),\n",
    "        point_labels=np.array([1]),\n",
    "        multimask_output=True,\n",
    "    )\n",
    "    mask = masks[scores.argmax()]"
   ],
   "id": "1b76a1dced69f131",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sommerl/PycharmProjects/od3d/venv_od3d/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T14:59:40.632349Z",
     "start_time": "2024-10-14T14:59:40.625082Z"
    }
   },
   "cell_type": "code",
   "source": "mask.shape",
   "id": "2210f88e7569ecb3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1920, 1440)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T14:59:44.028877Z",
     "start_time": "2024-10-14T14:59:43.989873Z"
    }
   },
   "cell_type": "code",
   "source": "show_img(torch.from_numpy(mask[None,]))",
   "id": "6bb0ecc092165d5c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T13:16:47.187566Z",
     "start_time": "2024-10-14T13:16:46.973188Z"
    }
   },
   "cell_type": "code",
   "source": "masks.shape",
   "id": "6d046b5ae6f73ecb",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mimg\u001B[49m\u001B[38;5;241m.\u001B[39mshape\n",
      "\u001B[0;31mNameError\u001B[0m: name 'img' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T12:58:04.916733Z",
     "start_time": "2024-10-14T12:58:04.812998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }"
   ],
   "id": "a88686a205f56c4c",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No points are provided; please add points first",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# run propagation throughout the video and collect the results in a dict\u001B[39;00m\n\u001B[1;32m      2\u001B[0m video_segments \u001B[38;5;241m=\u001B[39m {}  \u001B[38;5;66;03m# video_segments contains the per-frame segmentation results\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m out_frame_idx, out_obj_ids, out_mask_logits \u001B[38;5;129;01min\u001B[39;00m predictor\u001B[38;5;241m.\u001B[39mpropagate_in_video(inference_state):\n\u001B[1;32m      4\u001B[0m     video_segments[out_frame_idx] \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m      5\u001B[0m         out_obj_id: (out_mask_logits[i] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0.0\u001B[39m)\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m      6\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m i, out_obj_id \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(out_obj_ids)\n\u001B[1;32m      7\u001B[0m     }\n",
      "File \u001B[0;32m~/PycharmProjects/od3d/venv_od3d/lib/python3.10/site-packages/torch/utils/_contextlib.py:35\u001B[0m, in \u001B[0;36m_wrap_generator.<locals>.generator_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;66;03m# Issuing `None` to a generator fires it up\u001B[39;00m\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m---> 35\u001B[0m         response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m     38\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     39\u001B[0m             \u001B[38;5;66;03m# Forward the response to our caller and get its next request\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/od3d/venv_od3d/lib/python3.10/site-packages/sam2/sam2_video_predictor.py:663\u001B[0m, in \u001B[0;36mSAM2VideoPredictor.propagate_in_video\u001B[0;34m(self, inference_state, start_frame_idx, max_frame_num_to_track, reverse)\u001B[0m\n\u001B[1;32m    661\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_obj_num(inference_state)\n\u001B[1;32m    662\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(output_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcond_frame_outputs\u001B[39m\u001B[38;5;124m\"\u001B[39m]) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 663\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo points are provided; please add points first\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    664\u001B[0m clear_non_cond_mem \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclear_non_cond_mem_around_input \u001B[38;5;129;01mand\u001B[39;00m (\n\u001B[1;32m    665\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclear_non_cond_mem_for_multi_obj \u001B[38;5;129;01mor\u001B[39;00m batch_size \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    666\u001B[0m )\n\u001B[1;32m    668\u001B[0m \u001B[38;5;66;03m# set start index, end index, and processing order\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: No points are provided; please add points first"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T13:11:35.125783Z",
     "start_time": "2024-10-14T13:11:35.118501Z"
    }
   },
   "cell_type": "code",
   "source": "inference_state.keys()\n",
   "id": "c16c06fca3eddf11",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['images', 'num_frames', 'offload_video_to_cpu', 'offload_state_to_cpu', 'video_height', 'video_width', 'device', 'storage_device', 'point_inputs_per_obj', 'mask_inputs_per_obj', 'cached_features', 'constants', 'obj_id_to_idx', 'obj_idx_to_id', 'obj_ids', 'output_dict', 'output_dict_per_obj', 'temp_output_dict_per_obj', 'consolidated_frame_inds', 'tracking_has_started', 'frames_already_tracked'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T13:12:07.183473Z",
     "start_time": "2024-10-14T13:12:07.175831Z"
    }
   },
   "cell_type": "code",
   "source": "inference_state[\"obj_ids\"]",
   "id": "e13f94a55359688e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T10:18:51.102491Z",
     "start_time": "2024-10-14T10:18:51.096611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "cv2.imshow('img', img)\n",
    "#np.array(img)[:, :, ::-1].shape\n",
    "#plt.imshow(np.array(img)[:, :, :])"
   ],
   "id": "454a17e3771fd8d2",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    state = predictor.init_state(<your_video>)\n",
    "\n",
    "    # add new prompts and instantly get the output on the same frame\n",
    "    frame_idx, object_ids, masks = predictor.add_new_points_or_box(state, <your_prompts>):\n",
    "\n",
    "    # propagate the prompts to get masklets throughout the video\n",
    "    for frame_idx, object_ids, masks in predictor.propagate_in_video(state):"
   ],
   "id": "778c94e440e9450a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
